#https://www.kaggle.com/code/prashant111/xgboost-k-fold-cv-feature-importance/notebook

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt  # for plotting facilities

# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk(r'C:\John\git\vas\ML/data/'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output.
pass

data = r'C:\John\git\vas\ML/data/Wholesale customers data.csv'

df = pd.read_csv(data)

df.shape

df.head()

df.info()
pd.set_option('display.max_columns', None)

df.describe()

df.isnull().sum()

X = df.drop('Channel', axis=1)

y = df['Channel']

# convert labels into binary values

y[y == 2] = 0

y[y == 1] = 1

# import XGBoost
import xgboost as xgb


# define data_dmatrix
data_dmatrix = xgb.DMatrix(data=X,label=y)

# split X and y into training and testing sets

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)

# import XGBClassifier
from xgboost import XGBClassifier

# declare parameters
params = {
    'objective': 'binary:logistic',
    'max_depth': 4,
    'alpha': 10,
    'learning_rate': 1.0,
    'n_estimators': 100
}

# instantiate the classifier
xgb_clf = XGBClassifier(**params)

# fit the classifier to the training data
xgb_clf.fit(X_train, y_train)


# we can view the parameters of the xgb trained model as follows -

print(xgb_clf)


# make predictions on test data

y_pred = xgb_clf.predict(X_test)


from sklearn.metrics import accuracy_score

print('XGBoost model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))



from xgboost import cv

params = {"objective":"binary:logistic",'colsample_bytree': 0.3,'learning_rate': 0.1,
                'max_depth': 5, 'alpha': 10}

xgb_cv = cv(dtrain=data_dmatrix, params=params, nfold=3,
                    num_boost_round=50, early_stopping_rounds=10, metrics="auc", as_pandas=True, seed=123)


xgb.plot_importance(xgb_clf)
plt.figure(figsize = (16, 12))
plt.show()